{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:41.291789Z",
     "iopub.status.busy": "2024-04-11T04:24:41.290751Z",
     "iopub.status.idle": "2024-04-11T04:24:41.297706Z",
     "shell.execute_reply": "2024-04-11T04:24:41.296542Z",
     "shell.execute_reply.started": "2024-04-11T04:24:41.291728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention is all you need.\n",
    "# Transformers Model\n",
    "# Training a character based transformer model\n",
    "# https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T06:03:06.094515Z",
     "iopub.status.busy": "2024-04-11T06:03:06.093959Z",
     "iopub.status.idle": "2024-04-11T06:03:06.099052Z",
     "shell.execute_reply": "2024-04-11T06:03:06.097990Z",
     "shell.execute_reply.started": "2024-04-11T06:03:06.094488Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:42.402544Z",
     "iopub.status.busy": "2024-04-11T04:24:42.402223Z",
     "iopub.status.idle": "2024-04-11T04:24:42.409453Z",
     "shell.execute_reply": "2024-04-11T04:24:42.408682Z",
     "shell.execute_reply.started": "2024-04-11T04:24:42.402518Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/Users/nikhil20.sharma/Library/CloudStorage/GoogleDrive-nikhil.sharma1294@gmail.com/.shortcut-targets-by-id/1hJdN4IVIzv_akIWArIYt52MlRCEbxXvV/its.nikhilksharma/REPOs/Neural-Networks-Zero-to-Hero/data/tinyshakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:42.866885Z",
     "iopub.status.busy": "2024-04-11T04:24:42.866537Z",
     "iopub.status.idle": "2024-04-11T04:24:42.873406Z",
     "shell.execute_reply": "2024-04-11T04:24:42.872777Z",
     "shell.execute_reply.started": "2024-04-11T04:24:42.866858Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(file=file_path, mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:52.073688Z",
     "iopub.status.busy": "2024-04-11T04:24:52.073351Z",
     "iopub.status.idle": "2024-04-11T04:24:52.078040Z",
     "shell.execute_reply": "2024-04-11T04:24:52.077275Z",
     "shell.execute_reply.started": "2024-04-11T04:24:52.073660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[400:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T08:01:57.811630Z",
     "iopub.status.busy": "2024-04-11T08:01:57.811255Z",
     "iopub.status.idle": "2024-04-11T08:01:57.848855Z",
     "shell.execute_reply": "2024-04-11T08:01:57.848190Z",
     "shell.execute_reply.started": "2024-04-11T08:01:57.811600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Size of character vocab: 65\n",
      "\n",
      "[+] Characters available in the data\n",
      "[1 ]: '\\n'\t\t[2 ]: ' '\t\t[3 ]: '!'\t\t[4 ]: '$'\t\t[5 ]: '&'\t\t\n",
      "[6 ]: \"'\"\t\t[7 ]: ','\t\t[8 ]: '-'\t\t[9 ]: '.'\t\t[10]: '3'\t\t\n",
      "[11]: ':'\t\t[12]: ';'\t\t[13]: '?'\t\t[14]: 'A'\t\t[15]: 'B'\t\t\n",
      "[16]: 'C'\t\t[17]: 'D'\t\t[18]: 'E'\t\t[19]: 'F'\t\t[20]: 'G'\t\t\n",
      "[21]: 'H'\t\t[22]: 'I'\t\t[23]: 'J'\t\t[24]: 'K'\t\t[25]: 'L'\t\t\n",
      "[26]: 'M'\t\t[27]: 'N'\t\t[28]: 'O'\t\t[29]: 'P'\t\t[30]: 'Q'\t\t\n",
      "[31]: 'R'\t\t[32]: 'S'\t\t[33]: 'T'\t\t[34]: 'U'\t\t[35]: 'V'\t\t\n",
      "[36]: 'W'\t\t[37]: 'X'\t\t[38]: 'Y'\t\t[39]: 'Z'\t\t[40]: 'a'\t\t\n",
      "[41]: 'b'\t\t[42]: 'c'\t\t[43]: 'd'\t\t[44]: 'e'\t\t[45]: 'f'\t\t\n",
      "[46]: 'g'\t\t[47]: 'h'\t\t[48]: 'i'\t\t[49]: 'j'\t\t[50]: 'k'\t\t\n",
      "[51]: 'l'\t\t[52]: 'm'\t\t[53]: 'n'\t\t[54]: 'o'\t\t[55]: 'p'\t\t\n",
      "[56]: 'q'\t\t[57]: 'r'\t\t[58]: 's'\t\t[59]: 't'\t\t[60]: 'u'\t\t\n",
      "[61]: 'v'\t\t[62]: 'w'\t\t[63]: 'x'\t\t[64]: 'y'\t\t[65]: 'z'\t\t\n"
     ]
    }
   ],
   "source": [
    "char_vocab = sorted(list(set(text)))\n",
    "vocab_size = len(char_vocab)\n",
    "print(f'[+] Size of character vocab: {len(char_vocab)}\\n')\n",
    "print('[+] Characters available in the data')\n",
    "for  i, char in enumerate(char_vocab, start=1):\n",
    "    char = repr(char)\n",
    "    print(f'[{i:<2}]: {char}', end='\\t\\t')\n",
    "    if i%5 == 0: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:58.506886Z",
     "iopub.status.busy": "2024-04-11T04:24:58.506553Z",
     "iopub.status.idle": "2024-04-11T04:24:58.511952Z",
     "shell.execute_reply": "2024-04-11T04:24:58.511239Z",
     "shell.execute_reply.started": "2024-04-11T04:24:58.506862Z"
    }
   },
   "outputs": [],
   "source": [
    "global string_to_int, int_to_string\n",
    "\n",
    "string_to_int = {s:i for i, s in enumerate(char_vocab)}  # Encoding each string/char to int representation\n",
    "int_to_string = {i:s for i, s in enumerate(char_vocab)}  # Encoding each int to string/char representation\n",
    "\n",
    "def encode(string):\n",
    "    \"\"\"Encode a given string into integer sequence i.e. a list of integers.\"\"\"\n",
    "    return [string_to_int[c] for c in string if c in char_vocab]\n",
    "\n",
    "def decode(sequence):\n",
    "    \"\"\"Decode an integer sequence (list) into  the original string.\"\"\"\n",
    "    return ''.join([int_to_string[n] for n in sequence if n in int_to_string.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:24:58.929066Z",
     "iopub.status.busy": "2024-04-11T04:24:58.928766Z",
     "iopub.status.idle": "2024-04-11T04:24:58.933844Z",
     "shell.execute_reply": "2024-04-11T04:24:58.933060Z",
     "shell.execute_reply.started": "2024-04-11T04:24:58.929042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] test_string='Hi how are you doing, hope every thing is fine!'\n",
      "[+] Lenght of test string: 47\n",
      "[+] Encoding of test string: [20, 47, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59, 1, 42, 53, 47, 52, 45, 6, 1, 46, 53, 54, 43, 1, 43, 60, 43, 56, 63, 1, 58, 46, 47, 52, 45, 1, 47, 57, 1, 44, 47, 52, 43, 2]\n",
      "[+] Length of encoded test string: 47\n"
     ]
    }
   ],
   "source": [
    "test_string = 'Hi how are you doing, hope every thing is fine!'\n",
    "print(f'[+] {test_string=}')\n",
    "print(f'[+] Lenght of test string: {len(test_string)}')\n",
    "test_string_encoded = encode(string=test_string)\n",
    "print(f'[+] Encoding of test string: {test_string_encoded}')\n",
    "print(f'[+] Length of encoded test string: {len(test_string_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:25:00.258015Z",
     "iopub.status.busy": "2024-04-11T04:25:00.257726Z",
     "iopub.status.idle": "2024-04-11T04:25:00.261884Z",
     "shell.execute_reply": "2024-04-11T04:25:00.261116Z",
     "shell.execute_reply.started": "2024-04-11T04:25:00.257993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Length of encoded test string: 47\n",
      "[+] Decoded test string: Hi how are you doing, hope every thing is fine!\n",
      "[+] Length of decoded test string: 47\n"
     ]
    }
   ],
   "source": [
    "test_string_decoded = decode(sequence=test_string_encoded)\n",
    "print(f'[+] Length of encoded test string: {len(test_string_encoded)}')\n",
    "print(f'[+] Decoded test string: {test_string_decoded}')\n",
    "print(f'[+] Length of decoded test string: {len(test_string_decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Notes**\n",
    "1. Above we implemented the character level encoding for the sake of simplicity, but it is not used nowdays in the real world.\n",
    "2. Other methods like word-level or subword-level encoding are most commonly used instead.\n",
    "3. SentencePiece is widely used as compared to tikToken.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Tokenizer Name | Used/Developed by | URL                                                    | Notes                                   |\n",
    "|:---------------|:-----------------:|:-------------------------------------------------------|:----------------------------------------|\n",
    "|  SentencePiece |   Google          | [Github Repository](https://github.com/google/sentencepiece) <br> [Reference 1](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)| Sub-words(Byte-Pair-Encoding) <br> Character n-grams <br> Capture morphemes (smallest unit of maningful word in a language e.g. un-usual-ly) <br> Language independent <br> Fixed vocab size|\n",
    "|   tikToken     |   openAI          | [Repo](https://github.com/openai/tiktoken?tab=readme-ov-file) | Sub-words(Byte-Pair-Encoding), Language independent , Dynamic Vocab Size |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:28:49.084855Z",
     "iopub.status.busy": "2024-04-11T04:28:49.084554Z",
     "iopub.status.idle": "2024-04-11T04:28:49.792176Z",
     "shell.execute_reply": "2024-04-11T04:28:49.791703Z",
     "shell.execute_reply.started": "2024-04-11T04:28:49.084833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoding the entire text dataset and storing it in the torch.tensor object\n",
    "\n",
    "data = torch.tensor(data=encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:29:47.682903Z",
     "iopub.status.busy": "2024-04-11T04:29:47.682566Z",
     "iopub.status.idle": "2024-04-11T04:29:47.687366Z",
     "shell.execute_reply": "2024-04-11T04:29:47.686488Z",
     "shell.execute_reply.started": "2024-04-11T04:29:47.682878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] data.dtype=torch.int64\n",
      "[+] data.shape=torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "print(f'[+] {data.dtype=}')\n",
    "print(f'[+] {data.shape=}')\n",
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T04:40:04.065821Z",
     "iopub.status.busy": "2024-04-11T04:40:04.065268Z",
     "iopub.status.idle": "2024-04-11T04:40:04.072545Z",
     "shell.execute_reply": "2024-04-11T04:40:04.071755Z",
     "shell.execute_reply.started": "2024-04-11T04:40:04.065795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43, 56, 44, 50, 59, 47, 58,\n",
      "        63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1, 61, 43, 56, 43,  0, 61,\n",
      "        46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,  1, 51, 47, 45, 46, 58,\n",
      "         1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1, 56, 43, 50, 47, 43, 60,\n",
      "        43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43, 50, 63, 11,  0, 40, 59,\n",
      "        58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,  1, 61, 43,  1, 39, 56,\n",
      "        43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1, 58, 46, 43,  1, 50, 43,\n",
      "        39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0, 39, 44, 44, 50, 47, 41,\n",
      "        58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53, 40, 48, 43, 41, 58,  1,\n",
      "        53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56, 63,  6,  1, 47, 57,  1,\n",
      "        39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58, 53, 56, 63,  1, 58, 53,\n",
      "         1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47, 57, 43,  1, 58, 46, 43,\n",
      "        47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43, 11,  1, 53, 59, 56,  0,\n",
      "        57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47, 57,  1, 39,  1, 45, 39,\n",
      "        47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24, 43, 58,  1, 59, 57,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,  1, 61, 47, 58, 46,  0,\n",
      "        53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43, 56, 43,  1, 61, 43,  1,\n",
      "        40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57, 10,  1, 44, 53, 56,  1,\n",
      "        58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53, 61,  1, 21,  0, 57, 54,\n",
      "        43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1, 46, 59, 52, 45, 43, 56,\n",
      "         1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1, 52, 53, 58,  1, 47, 52,\n",
      "         1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1, 56, 43, 60, 43, 52, 45,\n",
      "        43,  8,  0,  0])\n",
      "\n",
      "but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[600:1000])\n",
    "print()\n",
    "print(decode(sequence=data[600:1000].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T05:23:09.950600Z",
     "iopub.status.busy": "2024-04-11T05:23:09.950271Z",
     "iopub.status.idle": "2024-04-11T05:23:09.959409Z",
     "shell.execute_reply": "2024-04-11T05:23:09.958672Z",
     "shell.execute_reply.started": "2024-04-11T05:23:09.950573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] len(train_data)=1003854\n",
      "[+] len(validation_data)=111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]\n",
    "\n",
    "print(f'[+] {len(train_data)=}')\n",
    "print(f'[+] {len(validation_data)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training data on a block size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T05:37:05.651102Z",
     "iopub.status.busy": "2024-04-11T05:37:05.650682Z",
     "iopub.status.idle": "2024-04-11T05:37:05.655697Z",
     "shell.execute_reply": "2024-04-11T05:37:05.654857Z",
     "shell.execute_reply.started": "2024-04-11T05:37:05.651047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum context length for prediction.\n",
    "# Maximum input that the model will ingest to generate the output.\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T05:31:47.221952Z",
     "iopub.status.busy": "2024-04-11T05:31:47.221477Z",
     "iopub.status.idle": "2024-04-11T05:31:47.226498Z",
     "shell.execute_reply": "2024-04-11T05:31:47.225783Z",
     "shell.execute_reply.started": "2024-04-11T05:31:47.221930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T05:33:05.656534Z",
     "iopub.status.busy": "2024-04-11T05:33:05.656218Z",
     "iopub.status.idle": "2024-04-11T05:33:05.663290Z",
     "shell.execute_reply": "2024-04-11T05:33:05.662674Z",
     "shell.execute_reply.started": "2024-04-11T05:33:05.656511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "y=tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "\n",
      "[+] Input --> Target | [18] --> 47\n",
      "[+] Input --> Target | [18, 47] --> 56\n",
      "[+] Input --> Target | [18, 47, 56] --> 57\n",
      "[+] Input --> Target | [18, 47, 56, 57] --> 58\n",
      "[+] Input --> Target | [18, 47, 56, 57, 58] --> 1\n",
      "[+] Input --> Target | [18, 47, 56, 57, 58, 1] --> 15\n",
      "[+] Input --> Target | [18, 47, 56, 57, 58, 1, 15] --> 47\n",
      "[+] Input --> Target | [18, 47, 56, 57, 58, 1, 15, 47] --> 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "print(f'{x=}')\n",
    "print(f'{y=}')\n",
    "print()\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'[+] Input --> Target | {context.tolist()} --> {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T06:06:24.470364Z",
     "iopub.status.busy": "2024-04-11T06:06:24.470072Z",
     "iopub.status.idle": "2024-04-11T06:06:24.476616Z",
     "shell.execute_reply": "2024-04-11T06:06:24.476004Z",
     "shell.execute_reply.started": "2024-04-11T06:06:24.470342Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# How many independent sequence will we process in parallel\n",
    "batch_size = 4\n",
    "\n",
    "# Maximum context length for prediction.\n",
    "# Maximum input that the model will ingest to generate the output.\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    '''Generate a small batch of data of inputsx and targets y'''\n",
    "\n",
    "    if split=='train':\n",
    "        data = train_data \n",
    "        print(f'[+] Generating \"training\" data...')\n",
    "    else:\n",
    "        data = validation_data\n",
    "        print(f'[+] Generating \"validation\" data...')\n",
    "\n",
    "    ix = torch.randint(\n",
    "        low=0,\n",
    "        high=len(data)-block_size,\n",
    "        size=(batch_size,)\n",
    "    )  # tensor([ 76049, 234249, 934904, 560986])\n",
    "\n",
    "    x = [torch.tensor(data[i:i+block_size]) for i in ix]\n",
    "    x = torch.stack(x)\n",
    "\n",
    "    y = [data[i+1:i+block_size+1] for i in ix]\n",
    "    y = torch.stack(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding training data with respect to batch and block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T06:15:00.010071Z",
     "iopub.status.busy": "2024-04-11T06:15:00.009815Z",
     "iopub.status.idle": "2024-04-11T06:15:00.014336Z",
     "shell.execute_reply": "2024-04-11T06:15:00.013644Z",
     "shell.execute_reply.started": "2024-04-11T06:15:00.010051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block Size: 1. Maximum context length for prediction.\n",
      "            2. Maximum input that the model will ingest to generate the output.\n",
      "            \n",
      "Batch Size: How many independent sequence will we process in parallel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Block Size: 1. Maximum context length for prediction.\n",
    "            2. Maximum input that the model will ingest to generate the output.\n",
    "            \n",
    "Batch Size: How many independent sequence will we process in parallel\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T06:06:39.367840Z",
     "iopub.status.busy": "2024-04-11T06:06:39.367583Z",
     "iopub.status.idle": "2024-04-11T06:06:39.373054Z",
     "shell.execute_reply": "2024-04-11T06:06:39.372469Z",
     "shell.execute_reply.started": "2024-04-11T06:06:39.367821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Generating \"training\" data...\n",
      "\n",
      "[+] xb.shape=torch.Size([4, 8])\n",
      "tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
      "\n",
      "[+] yb.shape=torch.Size([4, 8])\n",
      "tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print()\n",
    "print(f'[+] {xb.shape=}')\n",
    "print(xb)\n",
    "print()\n",
    "print(f'[+] {yb.shape=}')\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T06:34:29.357674Z",
     "iopub.status.busy": "2024-04-11T06:34:29.357375Z",
     "iopub.status.idle": "2024-04-11T06:34:29.367330Z",
     "shell.execute_reply": "2024-04-11T06:34:29.366668Z",
     "shell.execute_reply.started": "2024-04-11T06:34:29.357649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] batch=0\n",
      "\tblock=0 | [57] --> 43\n",
      "\tblock=1 | [57, 43] --> 60\n",
      "\tblock=2 | [57, 43, 60] --> 43\n",
      "\tblock=3 | [57, 43, 60, 43] --> 52\n",
      "\tblock=4 | [57, 43, 60, 43, 52] --> 1\n",
      "\tblock=5 | [57, 43, 60, 43, 52, 1] --> 63\n",
      "\tblock=6 | [57, 43, 60, 43, 52, 1, 63] --> 43\n",
      "\tblock=7 | [57, 43, 60, 43, 52, 1, 63, 43] --> 39\n",
      "\n",
      "[+] batch=1\n",
      "\tblock=0 | [60] --> 43\n",
      "\tblock=1 | [60, 43] --> 42\n",
      "\tblock=2 | [60, 43, 42] --> 8\n",
      "\tblock=3 | [60, 43, 42, 8] --> 0\n",
      "\tblock=4 | [60, 43, 42, 8, 0] --> 25\n",
      "\tblock=5 | [60, 43, 42, 8, 0, 25] --> 63\n",
      "\tblock=6 | [60, 43, 42, 8, 0, 25, 63] --> 1\n",
      "\tblock=7 | [60, 43, 42, 8, 0, 25, 63, 1] --> 45\n",
      "\n",
      "[+] batch=2\n",
      "\tblock=0 | [56] --> 42\n",
      "\tblock=1 | [56, 42] --> 5\n",
      "\tblock=2 | [56, 42, 5] --> 57\n",
      "\tblock=3 | [56, 42, 5, 57] --> 1\n",
      "\tblock=4 | [56, 42, 5, 57, 1] --> 57\n",
      "\tblock=5 | [56, 42, 5, 57, 1, 57] --> 39\n",
      "\tblock=6 | [56, 42, 5, 57, 1, 57, 39] --> 49\n",
      "\tblock=7 | [56, 42, 5, 57, 1, 57, 39, 49] --> 43\n",
      "\n",
      "[+] batch=3\n",
      "\tblock=0 | [43] --> 57\n",
      "\tblock=1 | [43, 57] --> 58\n",
      "\tblock=2 | [43, 57, 58] --> 63\n",
      "\tblock=3 | [43, 57, 58, 63] --> 6\n",
      "\tblock=4 | [43, 57, 58, 63, 6] --> 1\n",
      "\tblock=5 | [43, 57, 58, 63, 6, 1] --> 58\n",
      "\tblock=6 | [43, 57, 58, 63, 6, 1, 58] --> 46\n",
      "\tblock=7 | [43, 57, 58, 63, 6, 1, 58, 46] --> 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_size):\n",
    "    print(f'[+] {batch=}')\n",
    "    for block in range(block_size):\n",
    "        context = xb[batch, :block+1]\n",
    "        target = yb[batch, block]\n",
    "        print(f'\\t{block=} | {context.tolist()} --> {target}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_dev",
   "language": "python",
   "name": "gpt_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
